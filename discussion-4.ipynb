{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ce9a9d-56a1-41c6-9106-8c5dedac5806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a072d-3ecd-458f-9432-678864dfe1f2",
   "metadata": {},
   "source": [
    "### Enable GPU, (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc1472f-d8cf-4f5d-aef9-1a3e2fb429af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "if gpus:\n",
    "  try:    \n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_visible_devices(gpu, 'GPU')\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except Exception as e:\n",
    "    print(\"EXCEPTION\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef56e59-5ddd-43c3-87c8-a5777412ebda",
   "metadata": {},
   "source": [
    "### Load metadata from disk and determine image paths. Break data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef6c811-8d92-4256-b8d1-eccb46f4df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('./data/clothing/fashion.csv').sample(frac = 1)\n",
    "df_all['ImagePath'] = df_all.apply(lambda row: f'./data/clothing/{row.Category}/{row.Gender}/Images/images_with_product_ids/{row.Image}', axis = 1)\n",
    "df = df_all[:int(df_all.shape[0] * 0.7)]\n",
    "df_test = df_all[int(df_all.shape[0] * 0.7):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d49f6-1923-4538-a0a5-3c0c9b5510d8",
   "metadata": {},
   "source": [
    "### Initialize vectorizer for text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13758a7a-1499-416d-9464-27ad0786e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_titles = df['ProductTitle'].tolist()\n",
    "output_sequence_length = 32\n",
    "max_tokens = 1024\n",
    "\n",
    "vectorizer = keras.layers.TextVectorization(\n",
    "    max_tokens = max_tokens, output_mode = 'int', output_sequence_length = output_sequence_length\n",
    ")\n",
    "\n",
    "vectorizer.adapt([title for title in raw_titles])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2e5f3-3afa-4cdc-841b-b47241fcd022",
   "metadata": {},
   "source": [
    "### Prepare image and text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebb28ffb-d0ab-4d9c-aab1-7b6d54e5c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image_raw = tf.io.read_file(image_path)\n",
    "    image_tensor = tf.image.decode_jpeg(image_raw, channels = 3)\n",
    "    image_tensor = tf.image.convert_image_dtype(image_tensor, tf.float32)\n",
    "    image_resized =  tf.image.resize(image_tensor, [224, 224])\n",
    "    return image_resized\n",
    "    \n",
    "def featureize(df):\n",
    "    return (\n",
    "        tf.stack([vectorizer(title) for title in df['ProductTitle'].tolist()], axis = 0), \n",
    "        tf.stack([load_image(image_path) for image_path in df['ImagePath'].tolist()], axis = 0), \n",
    "        tf.constant((df['Category'] == 'Footwear').tolist())\n",
    "    )\n",
    "    \n",
    "training_titles, training_images, training_isfoot = featureize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee7211-77b2-46bd-a28e-b2ff19c203fd",
   "metadata": {},
   "source": [
    "### Define multi-input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c98635-87ae-4dd6-ba28-78f367a1e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_input = keras.Input(shape = (None,), dtype = tf.int32)\n",
    "x = keras.layers.Embedding(len(vectorizer.get_vocabulary()), output_sequence_length)(sentence_input)\n",
    "x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "sentence_output = keras.layers.Dense(32, activation = 'relu')(x)\n",
    "\n",
    "image_input = keras.Input(shape = (224, 224, 3))\n",
    "x = keras.layers.Rescaling(1.0 / 255)(image_input)\n",
    "x = keras.layers.Conv2D(192, kernel_size = 2, activation = 'relu')(x)\n",
    "x = keras.layers.MaxPooling2D(pool_size = (2, 2))(x)\n",
    "x = keras.layers.Conv2D(192, kernel_size = 2, activation = 'relu')(x)\n",
    "x = keras.layers.MaxPooling2D()(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "image_output = keras.layers.Dense(32, activation = 'relu')(x)\n",
    "\n",
    "merged = keras.layers.concatenate([sentence_output, image_output], axis = -1)\n",
    "output = keras.layers.Dense(1, activation = 'sigmoid')(merged)\n",
    "\n",
    "model = keras.models.Model([sentence_input, image_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d98cbf-fa79-4bb6-aca4-f7b671816c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1f56a4-7d17-445e-9242-2df6a2b6e578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 97ms/step - loss: 0.6032\n",
      "Epoch 2/4\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 99ms/step - loss: 0.0263\n",
      "Epoch 3/4\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 99ms/step - loss: 0.0045\n",
      "Epoch 4/4\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 99ms/step - loss: 0.0017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x31a68d8a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'crossentropy')\n",
    "model.fit([training_titles, training_images], training_isfoot, epochs = 4, batch_size = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b26f0e-721a-4efc-81ac-eb938be5621b",
   "metadata": {},
   "source": [
    "### Test model against test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfa5368a-ac37-4470-a250-347e02ee96db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 08:12:38.444967: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9988532110091743"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_titles, test_images, test_isfoot = featureize(df_test)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((test_titles, test_images, test_isfoot)).batch(10)\n",
    "test_results = 0\n",
    "i = 0\n",
    "\n",
    "for batch_titles, batch_images, batch_isfoot in dataset:\n",
    "    batch_results = model((batch_titles, batch_images), training = False) > 0.5\n",
    "    test_results += tf.math.count_nonzero(tf.reshape(batch_results, [-1]) == batch_isfoot)\n",
    "    i += batch_results.shape[0]\n",
    "\n",
    "(test_results / i).numpy().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
